---
title: "Monte Carlo error"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Let's construct a table of parameters leading to different Markov chains, each having the same marginal distribution $N(0,1)$ at the limit of large number of samples but each also having different amount of autocorrelation between the samples.

```{r}
varTheta = 1
sigma2.1 = 1
sigma2.3 = 0.2
phi.2 = 0.5
phi.4 = 0.1
phi.1 = sqrt(1-sigma2.1/varTheta)
phi.3 = sqrt(1-sigma2.3/varTheta)
sigma2.2 = varTheta*(1-phi.2^2)
sigma2.4 = varTheta*(1-phi.4^2)
table.entries = matrix(nrow=4, ncol=4, data=c(
  varTheta, phi.1, sigma2.1, phi.1,
  varTheta, phi.2, sigma2.2, phi.2,
  varTheta, phi.3, sigma2.3, phi.3,
  varTheta, phi.4, sigma2.4, phi.4
))
table.entries <- t(table.entries)  # take transpose since matrix fills in the elements in columnwise
colnames(table.entries) <- c("var(theta)", "phi", "sigma2","corr")
print(table.entries)

```

Let's then construct a function to perform Markov chain sampling

```{r}
# let's first define a function to conduct the sampling
MarkovChain <- function(phi,sigma2,initial,m){
  theta = vector(length=m)
  theta[1] = initial
  for (i1 in seq(1,m,1)){
    theta[i1+1] = phi*theta[i1] + rnorm(1,0,sqrt(sigma2))
  }
  return(theta)
}
```

Now we need to sample 100 independent realizations of length 2000 chains from the Markov chain defined in exercise 3.1 (that is; $\theta^{(1)},\dots, \theta^{(2000)}$) using each of the combinations of $\phi$ and $\sigma^2$ in the rows of the above table. 

With each of the chains we approximate $E[\theta^{(i)}]$, $\text{Pr}(\theta^{(i)}>0.5)$ and $\text{Pr}(\theta^{(i)}>2)$ using Monte Carlo with the $n=10$, $n=100$ and $n=1000$ last samples. Hence, we will construct 100 independent Monte Carlo approximations for the mean and two probabilities of $\theta$ corresponding to Markov chain sample sizes 10, 100 and 1000.

For example the below rows would construct two independent Markov chains of lenght 2000 and calculate the Monte Carlo approximation for the mean with the last 10 samples

```{r}
m <- 2000
n_combinations <- 4
n_chains <- 100

chains <- array(dim=c(n_combinations, n_chains, m+1))
dim(chains)

for (i in 1:n_combinations) {
  phi <- table.entries[i, 'phi']
  sigma2 <- table.entries[i, 'sigma2']
  initial <- runif(1, 0, 1000)
  
  for (j in 1:n_chains) {
    chain <- MarkovChain(phi, sigma2, initial, m)
    chains[i, j,] <- chain
  }
}
```

Now, we need to repeat the above steps 100 times, calculate the mean and asked probabilities for each of the 100 chains and then examine how these Monte Carlo estimates behave and match with the exact results as we vary the row of the table and $n$. 

```{r}
lengths <- c(10, 100, 1000)

par(mfrow=c(2, 2))
for (i in 1:n_combinations) {
  print(sprintf('Combination index: %d', i))
  # iterate through the chain lengths
  all_means <- array(dim=c(3, n_chains))
  all_p1s <- array(dim=c(3, n_chains))
  all_p2s <- array(dim=c(3, n_chains))

  for (j in 1:length(lengths)) {
    n <- lengths[j]
    #print(sprintf('Length: %d', j))
    means <- c()
    p1s <- c()
    p2s <- c()

    for (k in 1:n_chains) {
      #print(sprintf('Chain number: %d', k))
      #print(sprintf('m: %d, n: %d, (m-n+1):m = %d:%d', m, n, (m-n), m))
      samples <- chains[i,k,][(m-n+1):m]
      
      mean <- mean(samples)
      means <- c(mean, means)
      
      p1 <- sum(samples[samples > 0.5])/n
      p1s <- c(p1, p1s)

      p2 <- sum(samples[samples > 2])/n
      p2s <- c(p2, p2s)
      
    }
    #print(sprintf('Means length: %d', length(means)))
    all_means[j,] <- means
    all_p1s[j,] <- p1s
    all_p2s[j,] <- p2s
  }
  plots = list('n=10'=all_means, 'n=100'=p1s, 'n=100'=p2s)
  boxplot(plots)
}
```
